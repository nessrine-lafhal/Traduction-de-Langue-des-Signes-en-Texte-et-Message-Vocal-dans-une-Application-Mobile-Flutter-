{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    }
   ],
   "source": [
    "import cv2           \n",
    "import numpy as np   \n",
    "import os            \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import time          \n",
    "import mediapipe as mp  #pour la detection de corps\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialiser MediaPipe Holistic, qui est une solution pour la détection du corps entier\n",
    "mp_holistic = mp.solutions.holistic \n",
    "\n",
    "# Initialiser MediaPipe Drawing Utils, qui contient des utilitaires pour dessiner les annotations\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    # Convertir l'image de BGR en RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    \n",
    "    # Rendre l'image non modifiable (non writeable)\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    \n",
    "    # Faire la prédiction en utilisant le modèle MediaPipe\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    \n",
    "    # Rendre l'image modifiable à nouveau\n",
    "    image.flags.writeable = True                   # Image is now writeable\n",
    "    \n",
    "    # Convertir l'image de RGB en BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR CONVERSION RGB 2 BGR\n",
    "    \n",
    "    # Retourner l'image et les résultats de la prédiction\n",
    "    return image, results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    # Dessiner les connexions du visage\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    \n",
    "    # Dessiner les connexions de la pose (corps)\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    \n",
    "    # Dessiner les connexions de la main gauche\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    \n",
    "    # Dessiner les connexions de la main droite\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "\n",
    "    \n",
    "# draw_landmarks -> Function does not return the image but rather applies the landmark visualizations to the current image in place    \n",
    "\n",
    "\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Dessiner les connexions du visage\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             # Spécifications pour colorer les points de repère (points) du visage\n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             # Spécifications pour colorer les connexions (lignes) du visage\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Dessiner les connexions de la pose (corps)\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             # Spécifications pour colorer les points de repère (points) du corps\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             # Spécifications pour colorer les connexions (lignes) du corps\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Dessiner les connexions de la main gauche\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             # Spécifications pour colorer les points de repère (points) de la main gauche\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             # Spécifications pour colorer les connexions (lignes) de la main gauche\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Dessiner les connexions de la main droite  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             # Spécifications pour colorer les points de repère (points) de la main droite\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             # Spécifications pour colorer les connexions (lignes) de la main droite\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "\n",
    "    \n",
    "#we are just formatting or updating our draw landmark function(that's not complusory though, u can use only standard drwa_landmark func)    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialiser la capture vidéo à partir de la webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Définir le modèle MediaPipe Holistic avec des seuils de détection et de suivi\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        # Lire le flux vidéo\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            continue\n",
    "        \n",
    "        # Faire les détections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Dessiner les points de repère\n",
    "        draw_styled_landmarks(image, results)  # Utiliser la fonction améliorée pour dessiner les points de repère avec des couleurs différentes\n",
    "        \n",
    "        # Afficher à l'écran\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Quitter proprement\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Libérer la capture vidéo et fermer les fenêtres\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Compter le nombre de points de repère détectés sur la pose (corps)\n",
    "num_pose_landmarks = len(results.pose_landmarks.landmark) \n",
    "\n",
    "\n",
    "results\n",
    "\n",
    "\n",
    "# Appeler la fonction pour dessiner les points de repère sur le visage, le corps, et les mains\n",
    "draw_landmarks(frame, results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pose = []               \n",
    "# Pour chaque point de repère de la pose détecté, crée un tableau numpy avec les coordonnées x, y, z et la visibilité, puis ajoute-le à la liste 'pose'.\n",
    "for res in results.pose_landmarks.landmark:\n",
    "    test = np.array([res.x, res.y, res.z, res.visibility])\n",
    "    pose.append(test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Crée un tableau numpy plat avec les coordonnées x, y, z et la visibilité des points de repère de la pose; sinon, initialise un tableau de zéros de taille 132 si aucun point de repère n'est détecté.\n",
    "pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "\n",
    "# Crée un tableau numpy plat avec les coordonnées x, y, z des points de repère du visage; sinon, initialise un tableau de zéros de taille 1404 si aucun point de repère n'est détecté.\n",
    "face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "\n",
    "# Crée un tableau numpy plat avec les coordonnées x, y, z des points de repère de la main gauche; sinon, initialise un tableau de zéros de taille 63 si aucun point de repère n'est détecté.\n",
    "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "# Crée un tableau numpy plat avec les coordonnées x, y, z des points de repère de la main droite; sinon, initialise un tableau de zéros de taille 63 si aucun point de repère n'est détecté.\n",
    "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    # Crée un tableau numpy plat avec les coordonnées x, y, z et la visibilité des points de repère de la pose; sinon, initialise un tableau de zéros de taille 132 si aucun point de repère n'est détecté.\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    \n",
    "    # Crée un tableau numpy plat avec les coordonnées x, y, z des points de repère du visage; sinon, initialise un tableau de zéros de taille 1404 si aucun point de repère n'est détecté.\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    \n",
    "    # Crée un tableau numpy plat avec les coordonnées x, y, z des points de repère de la main gauche; sinon, initialise un tableau de zéros de taille 63 si aucun point de repère n'est détecté.\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    \n",
    "    # Crée un tableau numpy plat avec les coordonnées x, y, z des points de repère de la main droite; sinon, initialise un tableau de zéros de taille 63 si aucun point de repère n'est détecté.\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    \n",
    "    # Concatène tous les tableaux de points de repère (pose, visage, main gauche, main droite) en un seul tableau numpy.\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "\n",
    "\n",
    "result_test = extract_keypoints(results)\n",
    "\n",
    "\n",
    "\n",
    "np.save('0', result_test)   \n",
    "# en enregistrant chaque image sous forme de tableau numpy à l'intérieur de notre dossier 'MP_Data', nous aurons donc 30 tableaux numpy dans chaque dossier d'actions\n",
    "\n",
    "\n",
    "np.load('0.npy')\n",
    "\n",
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data') \n",
    "\n",
    "# Actions that we try to detect, including Tunisian Sign Language actions in French (transliterated)\n",
    "actions = np.array([ 'n3ass','lunettes','je froid','3aslama', 'Chokran', 'Nhebbek', 'La',  'j aime', 'j aime pas', 'Nakel', 'Nchrob' ,  'Bahi',\n",
    "      'Telephone',   's il te plait', 'pleur','fort', 'lissar','Waqtach',\n",
    "    'Limine' , 'sghir', 'kbir', 'stop', 'sma3ni','fakar','chof'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Trente vidéos de données\n",
    "no_sequences = 30\n",
    "\n",
    "# Les vidéos dureront 30 images\n",
    "sequence_length = 30\n",
    "\n",
    "\n",
    "# Vérifie si le chemin de répertoire spécifié par DATA_PATH n'existe pas.\n",
    "# Si le répertoire n'existe pas, il le crée.\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "\n",
    "\n",
    "\n",
    "# Itère sur chaque action dans la liste d'actions\n",
    "for action in actions: \n",
    "    # Pour chaque action, itère sur le nombre de séquences\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            # Essaye de créer un répertoire pour l'action et la séquence spécifiée dans DATA_PATH\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            # Ignore les erreurs (par exemple, si le répertoire existe déjà) et continue\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialise deux listes vides pour stocker les séquences et les étiquettes\n",
    "sequences, labels = [], []\n",
    "\n",
    "# Itère à travers chaque action dans la liste d'actions\n",
    "for action in actions:\n",
    "    # Pour chaque action, itère à travers chaque séquence (vidéo)\n",
    "    for sequence in range(no_sequences):\n",
    "        # Crée une liste pour stocker les frames de la séquence actuelle\n",
    "        window = []\n",
    "        # Itère à travers chaque frame de la séquence\n",
    "        for frame_num in range(sequence_length):\n",
    "            # Charge les points de repère extraits (au format .npy) pour la frame courante\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            # Ajoute les points de repère à la fenêtre (séquence)\n",
    "            window.append(res)\n",
    "        # Ajoute la fenêtre (séquence complète) à la liste des séquences\n",
    "        sequences.append(window)\n",
    "        # Ajoute l'étiquette correspondant à l'action à la liste des étiquettes\n",
    "        labels.append(label_map[action])\n",
    "\n",
    "\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nessrine\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 56ms/step - categorical_accuracy: 0.0342 - loss: 3.8584\n",
      "Epoch 2/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0408 - loss: 3.2757\n",
      "Epoch 3/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - categorical_accuracy: 0.0699 - loss: 3.2977\n",
      "Epoch 4/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - categorical_accuracy: 0.1071 - loss: 3.1587\n",
      "Epoch 5/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - categorical_accuracy: 0.0805 - loss: 3.1694\n",
      "Epoch 6/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0946 - loss: 3.2626\n",
      "Epoch 7/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0841 - loss: 3.3403\n",
      "Epoch 8/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - categorical_accuracy: 0.0920 - loss: 3.9779\n",
      "Epoch 9/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0481 - loss: 3.1853\n",
      "Epoch 10/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - categorical_accuracy: 0.0672 - loss: 3.1824\n",
      "Epoch 11/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - categorical_accuracy: 0.0678 - loss: 3.1897\n",
      "Epoch 12/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - categorical_accuracy: 0.0903 - loss: 5.3903\n",
      "Epoch 13/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0858 - loss: 4.4415\n",
      "Epoch 14/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0726 - loss: 11.9342\n",
      "Epoch 15/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0617 - loss: 8.9893\n",
      "Epoch 16/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0981 - loss: 3.6656\n",
      "Epoch 17/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0817 - loss: 3.3590\n",
      "Epoch 18/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0727 - loss: 3.3918\n",
      "Epoch 19/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0765 - loss: 4.1257\n",
      "Epoch 20/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.1055 - loss: 4.3599\n",
      "Epoch 21/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0514 - loss: 4.2193\n",
      "Epoch 22/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0786 - loss: 3.3914\n",
      "Epoch 23/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - categorical_accuracy: 0.0697 - loss: 3.6540\n",
      "Epoch 24/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.0844 - loss: 3.6818\n",
      "Epoch 25/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.0794 - loss: 6.4213\n",
      "Epoch 26/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0900 - loss: 4.1435\n",
      "Epoch 27/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0764 - loss: 3.1940\n",
      "Epoch 28/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.0863 - loss: 3.2234\n",
      "Epoch 29/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0924 - loss: 3.1039\n",
      "Epoch 30/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0991 - loss: 3.0960\n",
      "Epoch 31/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - categorical_accuracy: 0.0853 - loss: 3.0831\n",
      "Epoch 32/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0801 - loss: 3.1968\n",
      "Epoch 33/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.0839 - loss: 3.1480\n",
      "Epoch 34/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0882 - loss: 3.1625\n",
      "Epoch 35/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.0737 - loss: 3.1122\n",
      "Epoch 36/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0854 - loss: 3.2831\n",
      "Epoch 37/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0670 - loss: 3.3387\n",
      "Epoch 38/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0834 - loss: 3.1197\n",
      "Epoch 39/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.0787 - loss: 3.2439\n",
      "Epoch 40/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0694 - loss: 3.1042\n",
      "Epoch 41/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.0787 - loss: 3.1003\n",
      "Epoch 42/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - categorical_accuracy: 0.0873 - loss: 3.1070\n",
      "Epoch 43/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0894 - loss: 3.2201\n",
      "Epoch 44/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0844 - loss: 3.1781\n",
      "Epoch 45/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0647 - loss: 3.3081\n",
      "Epoch 46/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0920 - loss: 3.3038\n",
      "Epoch 47/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.0716 - loss: 3.2287\n",
      "Epoch 48/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.0663 - loss: 3.1107\n",
      "Epoch 49/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0752 - loss: 3.1670\n",
      "Epoch 50/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - categorical_accuracy: 0.0711 - loss: 3.2106\n",
      "Epoch 51/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0900 - loss: 3.2642\n",
      "Epoch 52/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.0913 - loss: 3.0885\n",
      "Epoch 53/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.0717 - loss: 3.0880\n",
      "Epoch 54/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0763 - loss: 3.2626\n",
      "Epoch 55/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0972 - loss: 3.0358\n",
      "Epoch 56/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.0913 - loss: 3.1001\n",
      "Epoch 57/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0831 - loss: 3.1145\n",
      "Epoch 58/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0765 - loss: 3.0865\n",
      "Epoch 59/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0779 - loss: 3.0595\n",
      "Epoch 60/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0775 - loss: 3.1037\n",
      "Epoch 61/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.0717 - loss: 3.2793\n",
      "Epoch 62/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0849 - loss: 3.0912\n",
      "Epoch 63/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - categorical_accuracy: 0.0763 - loss: 3.1036\n",
      "Epoch 64/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.0853 - loss: 3.0635\n",
      "Epoch 65/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.0743 - loss: 3.0484\n",
      "Epoch 66/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.0877 - loss: 3.0516\n",
      "Epoch 67/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.0758 - loss: 3.1135\n",
      "Epoch 68/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0692 - loss: 3.0799\n",
      "Epoch 69/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.0629 - loss: 3.1055\n",
      "Epoch 70/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.0723 - loss: 3.1805\n",
      "Epoch 71/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0932 - loss: 3.0669\n",
      "Epoch 72/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0796 - loss: 3.0867\n",
      "Epoch 73/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0749 - loss: 3.0910\n",
      "Epoch 74/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0667 - loss: 3.0946\n",
      "Epoch 75/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - categorical_accuracy: 0.0760 - loss: 3.0748\n",
      "Epoch 76/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - categorical_accuracy: 0.0812 - loss: 3.0667\n",
      "Epoch 77/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - categorical_accuracy: 0.0581 - loss: 3.0964\n",
      "Epoch 78/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - categorical_accuracy: 0.0800 - loss: 3.1089\n",
      "Epoch 79/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - categorical_accuracy: 0.0535 - loss: 3.6668\n",
      "Epoch 80/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - categorical_accuracy: 0.0636 - loss: 3.1735\n",
      "Epoch 81/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - categorical_accuracy: 0.0637 - loss: 3.5658\n",
      "Epoch 82/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - categorical_accuracy: 0.1182 - loss: 3.7017\n",
      "Epoch 83/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 96ms/step - categorical_accuracy: 0.0676 - loss: 3.8014\n",
      "Epoch 84/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - categorical_accuracy: 0.0915 - loss: 3.0569\n",
      "Epoch 85/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - categorical_accuracy: 0.0807 - loss: 3.4033\n",
      "Epoch 86/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 104ms/step - categorical_accuracy: 0.0858 - loss: 4.0181\n",
      "Epoch 87/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - categorical_accuracy: 0.0725 - loss: 4.0785\n",
      "Epoch 88/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - categorical_accuracy: 0.0945 - loss: 3.0718\n",
      "Epoch 89/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - categorical_accuracy: 0.0941 - loss: 3.0196\n",
      "Epoch 90/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - categorical_accuracy: 0.0873 - loss: 3.1014\n",
      "Epoch 91/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0913 - loss: 3.8416\n",
      "Epoch 92/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.1102 - loss: 3.0806\n",
      "Epoch 93/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - categorical_accuracy: 0.0793 - loss: 10.9640\n",
      "Epoch 94/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - categorical_accuracy: 0.0658 - loss: 11.3999\n",
      "Epoch 95/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0886 - loss: 3.1380\n",
      "Epoch 96/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0792 - loss: 3.2102\n",
      "Epoch 97/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - categorical_accuracy: 0.0950 - loss: 3.3229\n",
      "Epoch 98/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - categorical_accuracy: 0.0744 - loss: 3.4534\n",
      "Epoch 99/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - categorical_accuracy: 0.0779 - loss: 3.6186\n",
      "Epoch 100/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - categorical_accuracy: 0.0498 - loss: 4.7993\n",
      "Epoch 101/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.0769 - loss: 3.6720\n",
      "Epoch 102/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - categorical_accuracy: 0.0709 - loss: 3.3229\n",
      "Epoch 103/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.0825 - loss: 3.1220\n",
      "Epoch 104/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - categorical_accuracy: 0.0796 - loss: 3.2381\n",
      "Epoch 105/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0808 - loss: 3.1774\n",
      "Epoch 106/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.0796 - loss: 3.1998\n",
      "Epoch 107/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.0876 - loss: 3.0540\n",
      "Epoch 108/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0655 - loss: 3.1398\n",
      "Epoch 109/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - categorical_accuracy: 0.0743 - loss: 3.0673\n",
      "Epoch 110/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0719 - loss: 3.1400\n",
      "Epoch 111/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0791 - loss: 3.1260\n",
      "Epoch 112/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0949 - loss: 3.0808\n",
      "Epoch 113/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.0751 - loss: 3.2907\n",
      "Epoch 114/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0819 - loss: 3.2862\n",
      "Epoch 115/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - categorical_accuracy: 0.0879 - loss: 3.0900\n",
      "Epoch 116/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0955 - loss: 3.0819\n",
      "Epoch 117/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0855 - loss: 3.0944\n",
      "Epoch 118/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - categorical_accuracy: 0.0795 - loss: 3.0803\n",
      "Epoch 119/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0885 - loss: 3.0601\n",
      "Epoch 120/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0843 - loss: 3.0519\n",
      "Epoch 121/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0748 - loss: 3.0876\n",
      "Epoch 122/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0863 - loss: 3.0212\n",
      "Epoch 123/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0907 - loss: 3.0519\n",
      "Epoch 124/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0812 - loss: 3.0662\n",
      "Epoch 125/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0995 - loss: 3.0504\n",
      "Epoch 126/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.0883 - loss: 3.0385\n",
      "Epoch 127/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0932 - loss: 3.0248\n",
      "Epoch 128/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - categorical_accuracy: 0.0723 - loss: 3.0649\n",
      "Epoch 129/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - categorical_accuracy: 0.0740 - loss: 3.0740\n",
      "Epoch 130/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - categorical_accuracy: 0.1244 - loss: 3.0313\n",
      "Epoch 131/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0785 - loss: 3.0275\n",
      "Epoch 132/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0765 - loss: 3.0115\n",
      "Epoch 133/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0962 - loss: 2.9639\n",
      "Epoch 134/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0894 - loss: 3.0610\n",
      "Epoch 135/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - categorical_accuracy: 0.0739 - loss: 3.0448\n",
      "Epoch 136/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.0768 - loss: 3.0522\n",
      "Epoch 137/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0756 - loss: 3.0589\n",
      "Epoch 138/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - categorical_accuracy: 0.0894 - loss: 3.0663\n",
      "Epoch 139/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - categorical_accuracy: 0.1013 - loss: 3.0210\n",
      "Epoch 140/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0794 - loss: 3.0742\n",
      "Epoch 141/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.0874 - loss: 3.0371\n",
      "Epoch 142/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.0860 - loss: 3.0423\n",
      "Epoch 143/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.1075 - loss: 3.0077\n",
      "Epoch 144/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0933 - loss: 3.0260\n",
      "Epoch 145/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0908 - loss: 3.0462\n",
      "Epoch 146/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0921 - loss: 3.0368\n",
      "Epoch 147/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.1005 - loss: 2.9978\n",
      "Epoch 148/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0897 - loss: 3.0392\n",
      "Epoch 149/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - categorical_accuracy: 0.0854 - loss: 3.0154\n",
      "Epoch 150/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.0919 - loss: 3.0358\n",
      "Epoch 151/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0922 - loss: 3.0148\n",
      "Epoch 152/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - categorical_accuracy: 0.0975 - loss: 3.0105\n",
      "Epoch 153/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.0865 - loss: 3.0388\n",
      "Epoch 154/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0946 - loss: 3.0356\n",
      "Epoch 155/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.0845 - loss: 3.0852\n",
      "Epoch 156/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - categorical_accuracy: 0.0869 - loss: 3.0539\n",
      "Epoch 157/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - categorical_accuracy: 0.0774 - loss: 3.0670\n",
      "Epoch 158/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0865 - loss: 3.0637\n",
      "Epoch 159/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - categorical_accuracy: 0.0886 - loss: 3.0030\n",
      "Epoch 160/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0974 - loss: 3.0282\n",
      "Epoch 161/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0854 - loss: 3.0341\n",
      "Epoch 162/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.0883 - loss: 3.0513\n",
      "Epoch 163/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0662 - loss: 3.0837\n",
      "Epoch 164/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.0896 - loss: 3.0476\n",
      "Epoch 165/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - categorical_accuracy: 0.0755 - loss: 3.0471\n",
      "Epoch 166/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - categorical_accuracy: 0.0846 - loss: 3.0326\n",
      "Epoch 167/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0986 - loss: 3.0278\n",
      "Epoch 168/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.0757 - loss: 3.0556\n",
      "Epoch 169/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0830 - loss: 3.0548\n",
      "Epoch 170/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.0895 - loss: 3.0420\n",
      "Epoch 171/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - categorical_accuracy: 0.0887 - loss: 3.0504\n",
      "Epoch 172/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0775 - loss: 3.2353\n",
      "Epoch 173/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.0797 - loss: 3.2561\n",
      "Epoch 174/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - categorical_accuracy: 0.0889 - loss: 3.0672\n",
      "Epoch 175/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0837 - loss: 3.0364\n",
      "Epoch 176/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0875 - loss: 3.0290\n",
      "Epoch 177/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0813 - loss: 3.0936\n",
      "Epoch 178/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0665 - loss: 3.1926\n",
      "Epoch 179/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - categorical_accuracy: 0.0767 - loss: 3.1253\n",
      "Epoch 180/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0809 - loss: 3.0417\n",
      "Epoch 181/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0850 - loss: 3.0466\n",
      "Epoch 182/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0761 - loss: 3.0614\n",
      "Epoch 183/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - categorical_accuracy: 0.0900 - loss: 3.0522\n",
      "Epoch 184/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - categorical_accuracy: 0.0905 - loss: 3.0469\n",
      "Epoch 185/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - categorical_accuracy: 0.0987 - loss: 3.0418\n",
      "Epoch 186/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - categorical_accuracy: 0.0802 - loss: 3.0498\n",
      "Epoch 187/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - categorical_accuracy: 0.0791 - loss: 3.0576\n",
      "Epoch 188/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - categorical_accuracy: 0.0869 - loss: 3.0529\n",
      "Epoch 189/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0922 - loss: 3.0278\n",
      "Epoch 190/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - categorical_accuracy: 0.0816 - loss: 3.0336\n",
      "Epoch 191/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - categorical_accuracy: 0.0978 - loss: 3.0434\n",
      "Epoch 192/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0706 - loss: 3.0720\n",
      "Epoch 193/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - categorical_accuracy: 0.0923 - loss: 3.0261\n",
      "Epoch 194/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - categorical_accuracy: 0.0622 - loss: 3.0816\n",
      "Epoch 195/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - categorical_accuracy: 0.0837 - loss: 3.0557\n",
      "Epoch 196/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - categorical_accuracy: 0.0778 - loss: 3.0645\n",
      "Epoch 197/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - categorical_accuracy: 0.0921 - loss: 3.0461\n",
      "Epoch 198/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - categorical_accuracy: 0.0896 - loss: 3.0458\n",
      "Epoch 199/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - categorical_accuracy: 0.0865 - loss: 3.0243\n",
      "Epoch 200/200\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - categorical_accuracy: 0.0869 - loss: 3.0655\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">442,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">825</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_15 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │       \u001b[38;5;34m442,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_16 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m98,816\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_17 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │           \u001b[38;5;34m825\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,792,205</span> (6.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,792,205\u001b[0m (6.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">597,401</span> (2.28 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m597,401\u001b[0m (2.28 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,194,804</span> (4.56 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,194,804\u001b[0m (4.56 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "\n",
    "# Définit le chemin du répertoire où les journaux TensorBoard seront sauvegardés\n",
    "log_dir = os.path.join('Logs')\n",
    "\n",
    "# Crée une instance de TensorBoard pour enregistrer les journaux d'entraînement dans le répertoire spécifié\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "\n",
    "\n",
    "# Crée un modèle séquentiel\n",
    "model = Sequential()\n",
    "\n",
    "# Ajoute une couche LSTM avec 64 unités, renvoie les séquences (True) pour la connexion avec la couche suivante,\n",
    "# utilise la fonction d'activation ReLU et spécifie la forme d'entrée de (40, 1662)\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(40,1662)))\n",
    "\n",
    "# Ajoute une deuxième couche LSTM avec 128 unités, renvoie les séquences (True) pour la connexion avec la couche suivante,\n",
    "# utilise la fonction d'activation ReLU\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "\n",
    "# Ajoute une troisième couche LSTM avec 64 unités, ne renvoie pas les séquences (False),\n",
    "# utilise la fonction d'activation ReLU\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "\n",
    "# Ajoute une couche Dense avec 64 unités et la fonction d'activation ReLU\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Ajoute une couche Dense avec 32 unités et la fonction d'activation ReLU\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Ajoute une couche Dense finale avec un nombre d'unités égal au nombre d'actions,\n",
    "# avec la fonction d'activation softmax pour la classification multi-classe\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "# 'res' contient les probabilités de prédiction pour trois classes.\n",
    "# Les valeurs représentent la probabilité que l'entrée appartienne à chaque classe respective.\n",
    "# Ici, 0.7 indique la probabilité la plus élevée pour la première classe,\n",
    "# suivie de 0.2 pour la deuxième classe, et 0.1 pour la troisième classe.\n",
    "res = [.7, 0.2, 0.1]\n",
    "\n",
    "\n",
    "# Compile le modèle en spécifiant l'optimiseur, la fonction de perte et les métriques à utiliser pendant l'entraînement\n",
    "model.compile(\n",
    "    optimizer='Adam',                # Utilise l'optimiseur Adam, qui est un algorithme d'optimisation adaptatif couramment utilisé pour la formation des réseaux de neurones\n",
    "    loss='categorical_crossentropy', # Spécifie la fonction de perte comme l'entropie croisée catégorique, appropriée pour les problèmes de classification multi-classe où les étiquettes sont encodées en one-hot\n",
    "    metrics=['categorical_accuracy']  # Utilise la précision catégorique comme métrique d'évaluation pour suivre la performance du modèle pendant l'entraînement et l'évaluation\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=200, callbacks=[tb_callback])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "res = model.predict(X_test)\n",
    "\n",
    "model.save('action.h5')\n",
    "# Charge les poids préalablement sauvegardés dans le fichier 'action.h5' dans le modèle\n",
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "Accuracy: 0.05263157894736842\n",
      "Precision: 0.05263157894736842\n",
      "Recall (Sensitivity): 0.05263157894736842\n",
      "F1 Score: 0.05263157894736842\n",
      "Confusion Matrix:\n",
      " [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       0.00      0.00      0.00         2\n",
      "           5       0.00      0.00      0.00         2\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         3\n",
      "           8       0.00      0.00      0.00         2\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       0.00      0.00      0.00         2\n",
      "          11       0.00      0.00      0.00         3\n",
      "          12       0.00      0.00      0.00         2\n",
      "          13       0.00      0.00      0.00         1\n",
      "          14       1.00      1.00      1.00         2\n",
      "          15       0.00      0.00      0.00         2\n",
      "          16       0.00      0.00      0.00         2\n",
      "          17       0.00      0.00      0.00         0\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.00      0.00      0.00         4\n",
      "          20       0.00      0.00      0.00         2\n",
      "          21       0.00      0.00      0.00         1\n",
      "          22       0.00      0.00      0.00         1\n",
      "          23       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.05        38\n",
      "   macro avg       0.04      0.04      0.04        38\n",
      "weighted avg       0.05      0.05      0.05        38\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nessrine\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Nessrine\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Nessrine\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Nessrine\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Nessrine\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Nessrine\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Nessrine\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Nessrine\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "\n",
    "# convertit les prédictions de leur représentation codée à chaud en une étiquette catégorielle, par exemple 0,1,2 sous la forme [1,0,0],[0,1,0],[0,0,1]\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()\n",
    "\n",
    "multilabel_confusion_matrix(ytrue, yhat)\n",
    "\n",
    "accuracy_score(ytrue, yhat)\n",
    "\n",
    "\n",
    "# Calculer les métriques\n",
    "accuracy = accuracy_score(ytrue, yhat)\n",
    "precision = precision_score(ytrue, yhat, average='weighted')\n",
    "recall = recall_score(ytrue, yhat, average='weighted')\n",
    "f1 = f1_score(ytrue, yhat, average='weighted')\n",
    "conf_matrix = confusion_matrix(ytrue, yhat)\n",
    "\n",
    "\n",
    "\n",
    "# Afficher les métriques\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall (Sensitivity):\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(ytrue, yhat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.5160182   0.4524754  -1.0802364  ...  0.45198488  0.69363344\n",
      "   -0.00785414]\n",
      "  [ 0.5160037   0.4525295  -1.067734   ...  0.45351848  0.6975571\n",
      "   -0.01019249]\n",
      "  [ 0.5159935   0.45164716 -1.1196542  ...  0.45211238  0.69272876\n",
      "   -0.00923952]\n",
      "  ...\n",
      "  [ 0.51605225  0.4523972  -1.1294849  ...  0.4531132   0.69589317\n",
      "   -0.00204419]\n",
      "  [ 0.5160483   0.452478   -1.1026971  ...  0.4511778   0.7013948\n",
      "   -0.00705594]\n",
      "  [ 0.516053    0.45250475 -1.1198027  ...  0.45517576  0.700079\n",
      "   -0.00563149]]\n",
      "\n",
      " [[ 0.51374215  0.4563693  -1.1570764  ...  0.4525665   0.69688743\n",
      "   -0.00573525]\n",
      "  [ 0.5136515   0.4536169  -1.1316112  ...  0.44940856  0.68914217\n",
      "   -0.01146954]\n",
      "  [ 0.51458037  0.45280463 -1.1231663  ...  0.45195127  0.6892089\n",
      "   -0.0045726 ]\n",
      "  ...\n",
      "  [ 0.5159159   0.45236513 -1.1198636  ...  0.44984734  0.694088\n",
      "   -0.01629725]\n",
      "  [ 0.5159333   0.45235026 -1.1080568  ...  0.45124978  0.69292927\n",
      "   -0.00805763]\n",
      "  [ 0.51588714  0.45240363 -1.1216     ...  0.45065275  0.6950686\n",
      "   -0.00725913]]\n",
      "\n",
      " [[ 0.52241236  0.45396608 -1.1364727  ...  0.4520532   0.67838544\n",
      "    0.00231873]\n",
      "  [ 0.5224331   0.45367187 -1.111206   ...  0.45445147  0.6939788\n",
      "   -0.01188019]\n",
      "  [ 0.5224288   0.45369923 -1.1473771  ...  0.45459446  0.6897224\n",
      "   -0.010793  ]\n",
      "  ...\n",
      "  [ 0.5145025   0.45636234 -1.1422814  ...  0.45681912  0.7020977\n",
      "   -0.01082103]\n",
      "  [ 0.51403826  0.456489   -1.1404437  ...  0.4551862   0.6982701\n",
      "   -0.00622073]\n",
      "  [ 0.51382273  0.45645627 -1.1530037  ...  0.45239645  0.6973452\n",
      "   -0.00487254]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.456623    0.44518656 -1.0573227  ...  0.3268972   0.3980875\n",
      "   -0.03068542]\n",
      "  [ 0.45710975  0.44504228 -1.0594188  ...  0.3291869   0.39966\n",
      "   -0.03435595]\n",
      "  [ 0.4575246   0.44491735 -1.0569823  ...  0.3281995   0.40066144\n",
      "   -0.03644711]\n",
      "  ...\n",
      "  [ 0.4594434   0.44308585 -1.1253998  ...  0.32887274  0.40332738\n",
      "   -0.03893816]\n",
      "  [ 0.45947975  0.44315356 -1.0779659  ...  0.31891173  0.37667224\n",
      "   -0.04546165]\n",
      "  [ 0.4594923   0.4431973  -1.0720932  ...  0.32314396  0.3803653\n",
      "   -0.03120257]]\n",
      "\n",
      " [[ 0.41293877  0.43909484 -1.0355654  ...  0.29728225  0.401512\n",
      "   -0.02657351]\n",
      "  [ 0.4343788   0.43917987 -1.116364   ...  0.30285877  0.39009857\n",
      "   -0.0401889 ]\n",
      "  [ 0.44304663  0.4414026  -1.1289485  ...  0.31600407  0.4013343\n",
      "   -0.02479139]\n",
      "  ...\n",
      "  [ 0.45607394  0.44385615 -1.0173053  ...  0.32530594  0.4004488\n",
      "   -0.04145673]\n",
      "  [ 0.45636615  0.44466114 -1.0152403  ...  0.32720897  0.39814502\n",
      "   -0.0284088 ]\n",
      "  [ 0.45661598  0.4451927  -1.0136445  ...  0.32830277  0.39967167\n",
      "   -0.03219554]]\n",
      "\n",
      " [[ 0.47838745  0.4973446  -0.7504123  ...  0.32205075  0.4480121\n",
      "   -0.04099647]\n",
      "  [ 0.4754919   0.49906185 -0.91715443 ...  0.32973638  0.45664608\n",
      "   -0.03303052]\n",
      "  [ 0.47235766  0.49909335 -0.8626553  ...  0.33238783  0.46541706\n",
      "   -0.04220409]\n",
      "  ...\n",
      "  [ 0.41157714  0.43840486 -1.004425   ...  0.2850411   0.3934835\n",
      "   -0.03804641]\n",
      "  [ 0.41164693  0.43868625 -0.95675313 ...  0.28505915  0.39591536\n",
      "   -0.03233162]\n",
      "  [ 0.41173086  0.4389329  -0.9450928  ...  0.28810766  0.40060788\n",
      "   -0.03892969]]]\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "30\n",
      "['0.5972734093666077' '0.7213283777236938' '-1.5574215650558472' ...\n",
      " '0.45052751898765564' '-0.06592926383018494' 'def']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nessrine\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step\n",
      "fakar\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.04322484, 0.04232845, 0.04294547, 0.0400345 , 0.04105075,\n",
       "        0.04130815, 0.04245844, 0.03879448, 0.04043518, 0.04310024,\n",
       "        0.04106274, 0.03941403, 0.04042232, 0.03979805, 0.00578663,\n",
       "        0.04049407, 0.04022788, 0.04403084, 0.04240776, 0.03878455,\n",
       "        0.0412681 , 0.04107919, 0.04194118, 0.04368616, 0.04391601]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]* (len(actions) // 3 + 1)\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame\n",
    "# sequence.reverse()\n",
    "\n",
    "sequences = np.array(sequences, dtype=np.float32)  # Assurez-vous que toutes les séquences sont en float32\n",
    "sequences = sequences[::-1]  # Reverses the entire sequences array\n",
    "\n",
    "print(sequences)\n",
    "\n",
    "\n",
    "sequences = np.array(sequences, dtype=np.float32)  # Ensure all sequences are in float32\n",
    "for seq in sequences:\n",
    "    print(len(seq))\n",
    "\n",
    "\n",
    "# sequence.append('def')\n",
    "\n",
    "# Append 'def' to the numpy array\n",
    "sequence = np.append(sequence, 'def')\n",
    "print(sequence)\n",
    "\n",
    "\n",
    "#sequence.reverse()\n",
    "sequences = sequences[::-1] \n",
    "\n",
    "\n",
    "sequence[-40:]\n",
    "\n",
    "\n",
    "# 1. Variables pour la détection des gestes\n",
    "sequence = []            # Liste pour stocker les séquences de points clés\n",
    "sentence = []            # Liste pour stocker les actions détectées successivement\n",
    "predictions = []        # Liste pour stocker les prédictions du modèle\n",
    "threshold = 0.8          # Seuil de confiance pour valider une prédiction\n",
    "\n",
    "# Ouvrir la capture vidéo à partir de la webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialiser le modèle Mediapipe pour la détection des gestes\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        # Lire une image du flux vidéo\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Effectuer des détections sur l'image\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "        \n",
    "        # Dessiner les repères sur l'image\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Logique de prédiction\n",
    "        keypoints = extract_keypoints(results)  # Extraire les points clés de l'image\n",
    "        sequence.append(keypoints)             # Ajouter les points clés à la séquence\n",
    "        sequence = sequence[-20:]              # Conserver seulement les 20 derniers frames\n",
    "        \n",
    "        if len(sequence) == 20:                # Si la séquence contient 20 frames\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]  # Prédire l'action à partir de la séquence\n",
    "            print(actions[np.argmax(res)])    # Afficher l'action prédite\n",
    "            predictions.append(np.argmax(res))  # Ajouter la prédiction à la liste des prédictions\n",
    "            \n",
    "            # 3. Logique de visualisation\n",
    "            if np.unique(predictions[-10:])[0] == np.argmax(res):  # Si la prédiction dominante sur les 10 derniers frames est constante\n",
    "                if res[np.argmax(res)] > threshold:  # Vérifier si la probabilité est au-dessus du seuil\n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:  # Ajouter l'action à la sentence si elle est différente de la précédente\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])  # Ajouter la première action détectée à la sentence\n",
    "\n",
    "            if len(sentence) > 5:  # Limiter la taille de la sentence à 5 actions\n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Visualiser les probabilités de chaque action\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        # Dessiner un fond coloré et ajouter le texte de la sentence sur l'image\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,40), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Afficher l'image avec les visualisations\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Quitter la boucle lorsque la touche 'q' est pressée\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Libérer la capture vidéo et fermer toutes les fenêtres ouvertes\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "res[np.argmax(res)] > threshold\n",
    "\n",
    "model.predict(np.expand_dims(X_test[0], axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
